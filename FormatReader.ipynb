{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nGoal is to read and edit these file formats with their re-dacted counterparts\\n\\nSome of the most popular text files formats include:\\n\\n.DOC - This is a popular word processing file created by Microsoft Word. .DOC files are commonly used for producing written documents such as letters, CVs and essays.\\n.PAGES - Part of Apple's iWork suite, this is a text document and word processing file that is compatible with iOS and Mac OS.\\n.DOCX - After 2007, .DOCX became the standard file format for saving documents in Word. It enhances accessibility by zipping XML data into three different folders.\\n.MD - Commonly used by technical writers and web developers, .MD text files can create plain text documents with no other elements.\\n.EML - These are emails saved in plain text with no special formatting in the text, graphics or images.\\n.RTF - Rich text format files are universal text file formats that are more advanced than standard text files, capable of storing extra information and types of data.\\n.TXT - One of the most compatible and basic text file formats, .TXT documents are used to create simple text documents with little to no formatting.\\n.LOG - A log file is a data file that is used to track usage patterns, activities and operations within software and operating systems.\\n.ASC - These files are protected ASCII files that enable secure communication and data exchange.\\n.MSG - .MSG files are message text file formats that are used to represent individual email messages, contacts, tasks, appointments etc.\\n.WPS - The precursor to Microsoft Word DOCs, .WPS files were once the industry standard. They contain less advanced formatting and no macros.\\n\\n\\n\\nWith Extending our code to handle \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Goal is to read and edit these file formats with their re-dacted counterparts\n",
    "\n",
    "Some of the most popular text files formats include:\n",
    "\n",
    ".DOC - This is a popular word processing file created by Microsoft Word. .DOC files are commonly used for producing written documents such as letters, CVs and essays.\n",
    ".PAGES - Part of Apple's iWork suite, this is a text document and word processing file that is compatible with iOS and Mac OS.\n",
    ".DOCX - After 2007, .DOCX became the standard file format for saving documents in Word. It enhances accessibility by zipping XML data into three different folders.\n",
    ".MD - Commonly used by technical writers and web developers, .MD text files can create plain text documents with no other elements.\n",
    ".EML - These are emails saved in plain text with no special formatting in the text, graphics or images.\n",
    ".RTF - Rich text format files are universal text file formats that are more advanced than standard text files, capable of storing extra information and types of data.\n",
    ".TXT - One of the most compatible and basic text file formats, .TXT documents are used to create simple text documents with little to no formatting.\n",
    ".LOG - A log file is a data file that is used to track usage patterns, activities and operations within software and operating systems.\n",
    ".ASC - These files are protected ASCII files that enable secure communication and data exchange.\n",
    ".MSG - .MSG files are message text file formats that are used to represent individual email messages, contacts, tasks, appointments etc.\n",
    ".WPS - The precursor to Microsoft Word DOCs, .WPS files were once the industry standard. They contain less advanced formatting and no macros.\n",
    "\n",
    "\n",
    "\n",
    "With Extending our code to handle \n",
    ".csv via python builtins\n",
    ".doc via antiword\n",
    ".docx via python-docx2txt\n",
    ".eml via python builtins\n",
    ".epub via ebooklib\n",
    ".gif via tesseract-ocr\n",
    ".jpg and .jpeg via tesseract-ocr\n",
    ".json via python builtins\n",
    ".html and .htm via beautifulsoup4\n",
    ".mp3 via sox, SpeechRecognition, and pocketsphinx\n",
    ".msg via msg-extractor\n",
    ".odt via python builtins\n",
    ".ogg via sox, SpeechRecognition, and pocketsphinx\n",
    ".pdf via pdftotext (default) or pdfminer.six\n",
    ".png via tesseract-ocr\n",
    ".pptx via python-pptx\n",
    ".ps via ps2text\n",
    ".rtf via unrtf\n",
    ".tiff and .tif via tesseract-ocr\n",
    ".txt via python builtins\n",
    ".wav via SpeechRecognition and pocketsphinx\n",
    ".xlsx via xlrd\n",
    ".xls via xlrd\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting textextract\n",
      "  Downloading TextExtract-1.2.0.tar.gz (16.1 MB)\n",
      "     ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/16.1 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/16.1 MB 985.5 kB/s eta 0:00:16\n",
      "     - -------------------------------------- 0.8/16.1 MB 1.2 MB/s eta 0:00:14\n",
      "     -- ------------------------------------- 1.0/16.1 MB 1.1 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.3/16.1 MB 1.2 MB/s eta 0:00:13\n",
      "     --- ------------------------------------ 1.6/16.1 MB 1.2 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 1.8/16.1 MB 1.2 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 2.1/16.1 MB 1.2 MB/s eta 0:00:12\n",
      "     ----- ---------------------------------- 2.4/16.1 MB 1.2 MB/s eta 0:00:12\n",
      "     ------ --------------------------------- 2.6/16.1 MB 1.2 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 2.9/16.1 MB 1.2 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 3.1/16.1 MB 1.2 MB/s eta 0:00:11\n",
      "     -------- ------------------------------- 3.4/16.1 MB 1.2 MB/s eta 0:00:11\n",
      "     --------- ------------------------------ 3.7/16.1 MB 1.2 MB/s eta 0:00:11\n",
      "     --------- ------------------------------ 3.9/16.1 MB 1.2 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 4.2/16.1 MB 1.2 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 4.5/16.1 MB 1.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 4.5/16.1 MB 1.2 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 4.7/16.1 MB 1.2 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 5.0/16.1 MB 1.2 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 5.2/16.1 MB 1.2 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 5.5/16.1 MB 1.2 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 5.8/16.1 MB 1.2 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 6.0/16.1 MB 1.2 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 6.3/16.1 MB 1.2 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 6.6/16.1 MB 1.2 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 6.8/16.1 MB 1.2 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 7.1/16.1 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 7.3/16.1 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------------ --------------------- 7.6/16.1 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 7.9/16.1 MB 1.2 MB/s eta 0:00:07\n",
      "     -------------------- ------------------- 8.1/16.1 MB 1.2 MB/s eta 0:00:07\n",
      "     -------------------- ------------------- 8.4/16.1 MB 1.2 MB/s eta 0:00:07\n",
      "     --------------------- ------------------ 8.7/16.1 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------------- ----------------- 8.9/16.1 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------------- ----------------- 8.9/16.1 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------------- ----------------- 9.2/16.1 MB 1.2 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 9.4/16.1 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 9.7/16.1 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 10.0/16.1 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 10.2/16.1 MB 1.2 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 10.5/16.1 MB 1.2 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 10.7/16.1 MB 1.2 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 11.0/16.1 MB 1.2 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 11.3/16.1 MB 1.2 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 11.5/16.1 MB 1.2 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 11.8/16.1 MB 1.2 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 12.1/16.1 MB 1.2 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 12.3/16.1 MB 1.2 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.6/16.1 MB 1.2 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 12.8/16.1 MB 1.2 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 13.1/16.1 MB 1.2 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.4/16.1 MB 1.2 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.4/16.1 MB 1.2 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.6/16.1 MB 1.2 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 13.9/16.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 14.2/16.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 14.4/16.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 14.7/16.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 14.9/16.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 15.2/16.1 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.5/16.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.7/16.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.1/16.1 MB 1.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: textextract\n",
      "  Building wheel for textextract (setup.py): started\n",
      "  Building wheel for textextract (setup.py): finished with status 'done'\n",
      "  Created wheel for textextract: filename=TextExtract-1.2.0-py3-none-any.whl size=16081898 sha256=1bd204ae0addd880b28115116aa6c96378d65201abb68aac0e24df60e07b9f05\n",
      "  Stored in directory: C:\\Users\\vmuno\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-nk0yjpg3\\wheels\\be\\ce\\c7\\ec5f7d823cd4e911931c8f7eea018f0ced1992e866f60ee14a\n",
      "Successfully built textextract\n",
      "Installing collected packages: textextract\n",
      "Successfully installed textextract-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting textract\n",
      "  Downloading textract-1.6.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading textract-1.6.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring version 1.6.5 of textract since it has invalid metadata:\n",
      "Requested textract from https://files.pythonhosted.org/packages/6b/3e/ac16b6bf28edf78296aea7d0cb416b49ed30282ac8c711662541015ee6f3/textract-1.6.5-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [3 lines of output]\n",
      "      error in textract setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; .* suffix can only be used with `==` or `!=` operators\n",
      "          extract-msg<=0.29.*\n",
      "                     ~~~~~~~^\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textract'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextract\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textract'"
     ]
    }
   ],
   "source": [
    "import textract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docx.Document(\"Test00.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0x19bf3bc41a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<docx.text.paragraph.Paragraph at 0x19bf3c53e90>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c53e00>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32cc0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32bd0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32180>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c30470>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32960>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32d50>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32330>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c30c50>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32210>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32d80>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c326c0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32600>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32630>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32510>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c312b0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32750>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c327b0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c323c0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c327e0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32690>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c326f0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32b40>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c329f0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c31f70>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c324e0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c31af0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c329c0>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32b10>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c31820>,\n",
       " <docx.text.paragraph.Paragraph at 0x19bf3c32e40>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEDERAL BUREAU OF INVESTIGATION\n",
      "Report Number: FBI-2024-00123\n",
      "Date of Report: September 12, 2024\n",
      "Case Number: 1234567890\n",
      "\n",
      "SUBJECT(S):\n",
      "Name: John Doe\n",
      "Date of Birth: January 1, 1985\n",
      "Sex: Male\n",
      "Race: Caucasian\n",
      "Height: 6'1\"\n",
      "Weight: 185 lbs\n",
      "Hair: Brown\n",
      "Eyes: Blue\n",
      "Known Aliases: N/A\n",
      "\n",
      "CASE SUMMARY:\n",
      "On September 5, 2024, a complaint was filed with the FBI's Cyber Crime Division regarding an alleged cyber attack targeting the financial infrastructure of [Bank X]. The attack resulted in unauthorized access to sensitive customer information, including personal identifying information (PII).\n",
      "Preliminary investigations led to the identification of IP addresses linked to a group of individuals, one of whom has been identified as the primary suspect, John Doe.\n",
      "\n",
      "INVESTIGATION DETAILS:\n",
      "Initial Complaint:\n",
      "The report was filed by Bank X's IT security team at 10:30 AM on September 5, 2024. They reported suspicious activity on their internal network, including unauthorized access to client databases.\n",
      "Evidence Collected:\n",
      "Log files from Bank X's server showing multiple login attempts from unknown IP addresses.\n",
      "Malware analysis of a trojan installed on the server, believed to be part of the attack.\n",
      "Network traffic logs indicating data exfiltration attempts between September 3 and September 4, 2024.\n",
      "Identification of a suspicious IP address linked to the suspect, John Doe.\n",
      "Surveillance:\n",
      "Physical and digital surveillance of John Doe was initiated on September 6, 2024. During this period, Doe was observed meeting with known associates involved in prior cybercrime activities.\n",
      "Search and Seizure:\n",
      "On September 9, 2024, a federal search warrant was executed at John Doe's residence, resulting in the seizure of:\n",
      "One (1) desktop computer\n",
      "Two (2) smartphones\n",
      "One (1) external hard drive containing potentially stolen data\n",
      "Interrogation:\n",
      "John Doe was detained for questioning on September 10, 2024. During the interview, Doe denied involvement in the cyber attack. He claimed the devices seized were for personal use only. However, digital forensics indicates the presence of tools commonly used in cybercrime activities.\n",
      "\n",
      "NEXT STEPS:\n",
      "Forensic Analysis: The devices seized from Doe’s residence are currently undergoing forensic analysis to recover any deleted files or encrypted data that may provide further evidence of his involvement.\n",
      "Warrant Requests: Further search warrants are being requested for the suspect's financial records and communication logs to establish a possible connection to the stolen data.\n",
      "Further Interrogation: A second round of interrogation is scheduled to confront Doe with newly uncovered evidence.\n",
      "\n",
      "CONCLUSION:\n",
      "Based on the evidence collected to date, John Doe remains the prime suspect in the cyber attack against Bank X. Further investigation is ongoing to solidify the connection between the suspect and the financial data breach.\n",
      "\n",
      "INVESTIGATING OFFICER:\n",
      "Special Agent Jane Smith\n",
      "Cyber Crime Division\n",
      "Federal Bureau of Investigation\n",
      "Contact: (202) 555-0123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in doc.paragraphs:\n",
    "    print (para.text)\n",
    "    para.text = \"Edited\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n",
      "Edited\n"
     ]
    }
   ],
   "source": [
    "for para in doc.paragraphs:\n",
    "    print (para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Redacting:  Test00.docx\n",
      "FEDERAL BUREAU OF INVESTIGATION\n",
      "Report Number: FBI-2024-00123\n",
      "Date of Report: September 12, 2024\n",
      "Case Number: 1234567890\n",
      "\n",
      "SUBJECT(S):\n",
      "Name: John Doe\n",
      "Date of Birth: January 1, 1985\n",
      "Sex: Male\n",
      "Race: Caucasian\n",
      "Height: 6'1\"\n",
      "Weight: 185 lbs\n",
      "Hair: Brown\n",
      "Eyes: Blue\n",
      "Known Aliases: N/A\n",
      "\n",
      "CASE SUMMARY:\n",
      "On September 5, 2024, a complaint was filed with the FBI's Cyber Crime Division regarding an alleged cyber attack targeting the financial infrastructure of [Bank X]. The attack resulted in unauthorized access to sensitive customer information, including personal identifying information (PII).\n",
      "Preliminary investigations led to the identification of IP addresses linked to a group of individuals, one of whom has been identified as the primary suspect, John Doe.\n",
      "\n",
      "INVESTIGATION DETAILS:\n",
      "Initial Complaint:\n",
      "The report was filed by Bank X's IT security team at 10:30 AM on September 5, 2024. They reported suspicious activity on their internal network, including unauthorized access to client databases.\n",
      "Evidence Collected:\n",
      "Log files from Bank X's server showing multiple login attempts from unknown IP addresses.\n",
      "Malware analysis of a trojan installed on the server, believed to be part of the attack.\n",
      "Network traffic logs indicating data exfiltration attempts between September 3 and September 4, 2024.\n",
      "Identification of a suspicious IP address linked to the suspect, John Doe.\n",
      "Surveillance:\n",
      "Physical and digital surveillance of John Doe was initiated on September 6, 2024. During this period, Doe was observed meeting with known associates involved in prior cybercrime activities.\n",
      "Search and Seizure:\n",
      "On September 9, 2024, a federal search warrant was executed at John Doe's residence, resulting in the seizure of:\n",
      "One (1) desktop computer\n",
      "Two (2) smartphones\n",
      "One (1) external hard drive containing potentially stolen data\n",
      "Interrogation:\n",
      "John Doe was detained for questioning on September 10, 2024. During the interview, Doe denied involvement in the cyber attack. He claimed the devices seized were for personal use only. However, digital forensics indicates the presence of tools commonly used in cybercrime activities.\n",
      "\n",
      "NEXT STEPS:\n",
      "Forensic Analysis: The devices seized from Doe’s residence are currently undergoing forensic analysis to recover any deleted files or encrypted data that may provide further evidence of his involvement.\n",
      "Warrant Requests: Further search warrants are being requested for the suspect's financial records and communication logs to establish a possible connection to the stolen data.\n",
      "Further Interrogation: A second round of interrogation is scheduled to confront Doe with newly uncovered evidence.\n",
      "\n",
      "CONCLUSION:\n",
      "Based on the evidence collected to date, John Doe remains the prime suspect in the cyber attack against Bank X. Further investigation is ongoing to solidify the connection between the suspect and the financial data breach.\n",
      "\n",
      "INVESTIGATING OFFICER:\n",
      "Special Agent Jane Smith\n",
      "Cyber Crime Division\n",
      "Federal Bureau of Investigation\n",
      "Contact: (202) 555-0123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import os\n",
    "\n",
    "for file in os.listdir(\"input\"):\n",
    "    if (\".docx\" in file):\n",
    "        doc = docx.Document(\"input/\" + file)\n",
    "        print (\"Now Redacting: \", file)\n",
    "        for para in doc.paragraphs:\n",
    "            print(para.text)\n",
    "            # redact now\n",
    "\n",
    "        # Saving the redacted Document\n",
    "        print(\"Now Saving: \", file)\n",
    "        doc.save(\"output/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "doc.save(\"Redacted.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "file 'Test00.doc' is not a Word file, content type is 'application/vnd.openxmlformats-officedocument.themeManager+xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mdocx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest00.doc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\docx\\api.py:30\u001b[0m, in \u001b[0;36mDocument\u001b[1;34m(docx)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mcontent_type \u001b[38;5;241m!=\u001b[39m CT\u001b[38;5;241m.\u001b[39mWML_DOCUMENT_MAIN:\n\u001b[0;32m     29\u001b[0m     tmpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a Word file, content type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(tmpl \u001b[38;5;241m%\u001b[39m (docx, document_part\u001b[38;5;241m.\u001b[39mcontent_type))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m document_part\u001b[38;5;241m.\u001b[39mdocument\n",
      "\u001b[1;31mValueError\u001b[0m: file 'Test00.doc' is not a Word file, content type is 'application/vnd.openxmlformats-officedocument.themeManager+xml'"
     ]
    }
   ],
   "source": [
    "doc = docx.Document(\"Test00.doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Redacting:  Test00.docx\n",
      "FEDERAL BUREAU OF INVESTIGATION\n",
      "Report Number: FBI-2024-00123\n",
      "Date of Report: September 12, 2024\n",
      "Case Number: 1234567890\n",
      "\n",
      "SUBJECT(S):\n",
      "Name: John Doe\n",
      "Date of Birth: January 1, 1985\n",
      "Sex: Male\n",
      "Race: Caucasian\n",
      "Height: 6'1\"\n",
      "Weight: 185 lbs\n",
      "Hair: Brown\n",
      "Eyes: Blue\n",
      "Known Aliases: N/A\n",
      "\n",
      "CASE SUMMARY:\n",
      "On September 5, 2024, a complaint was filed with the FBI's Cyber Crime Division regarding an alleged cyber attack targeting the financial infrastructure of [Bank X]. The attack resulted in unauthorized access to sensitive customer information, including personal identifying information (PII).\n",
      "Preliminary investigations led to the identification of IP addresses linked to a group of individuals, one of whom has been identified as the primary suspect, John Doe.\n",
      "\n",
      "INVESTIGATION DETAILS:\n",
      "Initial Complaint:\n",
      "The report was filed by Bank X's IT security team at 10:30 AM on September 5, 2024. They reported suspicious activity on their internal network, including unauthorized access to client databases.\n",
      "Evidence Collected:\n",
      "Log files from Bank X's server showing multiple login attempts from unknown IP addresses.\n",
      "Malware analysis of a trojan installed on the server, believed to be part of the attack.\n",
      "Network traffic logs indicating data exfiltration attempts between September 3 and September 4, 2024.\n",
      "Identification of a suspicious IP address linked to the suspect, John Doe.\n",
      "Surveillance:\n",
      "Physical and digital surveillance of John Doe was initiated on September 6, 2024. During this period, Doe was observed meeting with known associates involved in prior cybercrime activities.\n",
      "Search and Seizure:\n",
      "On September 9, 2024, a federal search warrant was executed at John Doe's residence, resulting in the seizure of:\n",
      "One (1) desktop computer\n",
      "Two (2) smartphones\n",
      "One (1) external hard drive containing potentially stolen data\n",
      "Interrogation:\n",
      "John Doe was detained for questioning on September 10, 2024. During the interview, Doe denied involvement in the cyber attack. He claimed the devices seized were for personal use only. However, digital forensics indicates the presence of tools commonly used in cybercrime activities.\n",
      "\n",
      "NEXT STEPS:\n",
      "Forensic Analysis: The devices seized from Doe’s residence are currently undergoing forensic analysis to recover any deleted files or encrypted data that may provide further evidence of his involvement.\n",
      "Warrant Requests: Further search warrants are being requested for the suspect's financial records and communication logs to establish a possible connection to the stolen data.\n",
      "Further Interrogation: A second round of interrogation is scheduled to confront Doe with newly uncovered evidence.\n",
      "\n",
      "CONCLUSION:\n",
      "Based on the evidence collected to date, John Doe remains the prime suspect in the cyber attack against Bank X. Further investigation is ongoing to solidify the connection between the suspect and the financial data breach.\n",
      "\n",
      "INVESTIGATING OFFICER:\n",
      "Special Agent Jane Smith\n",
      "Cyber Crime Division\n",
      "Federal Bureau of Investigation\n",
      "Contact: (202) 555-0123\n",
      "\n",
      "Now Saving:  Test00.docx\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import os\n",
    "\n",
    "for file in os.listdir(\"input\"):\n",
    "    if (\".docx\" in file):\n",
    "        doc = docx.Document(\"input/\" + file)\n",
    "        print (\"Now Redacting: \", file)\n",
    "        for para in doc.paragraphs:\n",
    "            print(para.text)\n",
    "            # redact now\n",
    "\n",
    "        # Saving the redacted Document\n",
    "        print(\"Now Saving: \", file)\n",
    "        doc.save(\"output/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vmuno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\vmuno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vmuno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vmuno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe Date', 'Caucasian Height', 'Brown', 'Blue Known', 'John Doe', 'Evidence', 'Malware', 'Network', 'John Doe', 'John Doe', 'Doe', 'Search', 'Seizure', 'John Doe', 'John Doe', 'Doe', 'Doe', 'John Doe', 'Further', 'Jane Smith']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def redact_names(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Apply POS tagging to the tokens\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Perform Named Entity Recognition (NER)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "    # print (named_entities)\n",
    "\n",
    "    # Redact names (PERSON entities)\n",
    "    redacted_text = []\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "            # print (\"Label: \", chunk)\n",
    "            redacted_text.append(\" \".join(c[0] for c in chunk))\n",
    "       \n",
    "    return (redacted_text)\n",
    "\n",
    "#Sample text\n",
    "with open(\"Test00.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Redact names\n",
    "redacted_text = redact_names(text)\n",
    "print(redacted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\vmuno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\vmuno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Brown', 'Caucasian', 'Smith', 'Height', 'Malware', 'Blue', 'Evidence', 'Search', 'Further', 'Jane', 'Seizure', 'Known', 'Date', 'Network', 'John', 'Doe'}\n"
     ]
    }
   ],
   "source": [
    "from Redactor import *\n",
    "import os\n",
    "\n",
    "all_files = os.listdir(\"Input/\")\n",
    "\n",
    "files = []\n",
    "\n",
    "for file in all_files:\n",
    "    if (\".docx\" in file or \".txt\" in file or \".md\" in file or \".doc\" in file):\n",
    "        files.append(file)\n",
    "\n",
    "with open(files[2]) as file:\n",
    "    text = file.read()\n",
    "\n",
    "redacted_text = Redactor.redact_names(text)\n",
    "redacted_words = []\n",
    "for phrases in redacted_text:\n",
    "    redacted_words+=phrases.split(\" \")\n",
    "redacted_words = set(redacted_words)\n",
    "print(redacted_words)\n",
    "optext=[]\n",
    "\n",
    "with open(files[2]) as file:\n",
    "    i=0\n",
    "    text=file.readline()\n",
    "    text=text[:len(text)-1]\n",
    "    while (text):\n",
    "        opttextLine=[]\n",
    "\n",
    "        for word in text.split(\" \"):\n",
    "            for redacted_word in redacted_words:\n",
    "                if (redacted_word in word):\n",
    "                    if (word[-1]=='\\n'):\n",
    "                        word=\"[REDACTED]\\n\"\n",
    "                    else:\n",
    "                        word=\"[REDACTED]\"\n",
    "            opttextLine.append(word)\n",
    "        optext.append(opttextLine)\n",
    "        text=file.readline()\n",
    "        i+=1\n",
    "file_data=\"\"\n",
    "for lines in optext:\n",
    "    for word in lines:\n",
    "        file_data += word+\" \"\n",
    "with open(\"output/\"+files[2], \"w\") as file:\n",
    "    file.write(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe Date', 'Caucasian Height', 'Brown', 'Blue Known', 'John Doe', 'Evidence', 'Malware', 'Network', 'John Doe', 'John Doe', 'Doe', 'Search', 'Seizure', 'John Doe', 'John Doe', 'Doe', 'Doe', 'John Doe', 'Further', 'Jane Smith']\n"
     ]
    }
   ],
   "source": [
    "from Redactor import *\n",
    "import os\n",
    "\n",
    "with open('common_words.txt') as file:\n",
    "    common_words = list(file.readlines())\n",
    "\n",
    "all_files = os.listdir(\"Input/\")\n",
    "\n",
    "files = []\n",
    "\n",
    "for file in all_files:\n",
    "    if (\".docx\" in file or \".txt\" in file or \".md\" in file or \".doc\" in file):\n",
    "        files.append(file)\n",
    "\n",
    "with open(files[2]) as file:\n",
    "    text = file.read()\n",
    "\n",
    "redacted_words = Redactor.redact_names(text)\n",
    "\n",
    "print(redacted_words)\n",
    "optext=[]\n",
    "\n",
    "with open(files[2]) as file:\n",
    "    i=0\n",
    "    text=file.readline()\n",
    "    text=text[:len(text)-1]\n",
    "    while (text):\n",
    "        opttextLine=[]\n",
    "\n",
    "        for word in text.split(\" \"):\n",
    "            for redacted_word in redacted_words:\n",
    "                if (redacted_word in word):\n",
    "                    if (word[-1]=='\\n'):\n",
    "                        word=\"[REDACTED]\\n\"\n",
    "                    else:\n",
    "                        word=\"[REDACTED]\"\n",
    "            opttextLine.append(word)\n",
    "        optext.append(opttextLine)\n",
    "        text=file.readline()\n",
    "        i+=1\n",
    "file_data=\"\"\n",
    "for lines in optext:\n",
    "    for word in lines:\n",
    "        file_data += word+\" \"\n",
    "with open(\"output/\"+files[2], \"w\") as file:\n",
    "    file.write(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Your JSON data (as a Python list of dictionaries)\n",
    "with open('dictionary_alpha_arrays.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "common_words = []\n",
    "\n",
    "# Extract and print key items\n",
    "for entry in data:\n",
    "    for word, definition in entry.items():\n",
    "        common_words.append(word)\n",
    "\n",
    "common_words\n",
    "\n",
    "with open(\"common_words.txt\", 'w' ) as file:\n",
    "    file.write(''.join(x +'\\n' for x in common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102104\n",
      "further\n",
      "jane\n",
      "doe\n",
      "caucasian\n",
      "john\n",
      "height\n",
      "seizure\n",
      "known\n",
      "date\n",
      "blue\n",
      "brown\n",
      "search\n",
      "network\n",
      "evidence\n",
      "smith\n",
      "malware\n",
      "['Further', 'Jane', 'Doe', 'Caucasian', 'John', 'Height', 'Seizure', 'Known', 'Date', 'Blue', 'Brown', 'Search', 'Network', 'Evidence', 'Smith', 'Malware']\n"
     ]
    }
   ],
   "source": [
    "def redact_names(text) -> list:\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Apply POS tagging to the tokens\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    # Perform Named Entity Recognition (NER)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "    # print (named_entities)\n",
    "\n",
    "    # Redact names (PERSON entities)\n",
    "    redacted_text = []\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "            # print (\"Label: \", chunk)\n",
    "            redacted_text.append(\" \".join(c[0] for c in chunk))\n",
    "\n",
    "    with open('common_words.txt') as file:\n",
    "        common_words = file.readlines()\n",
    "    print(len(common_words))\n",
    "\n",
    "    redacted_words = []\n",
    "    for phrases in redacted_text:\n",
    "        redacted_words+=phrases.split(\" \")\n",
    "    redacted_words = list(set(redacted_words))\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for word in redacted_words:\n",
    "        if (word.lower() not in common_words):\n",
    "            print(word.lower())\n",
    "            result.append(word)\n",
    "    \n",
    "    return (result)\n",
    "\n",
    "for file in all_files:\n",
    "    if (\".docx\" in file or \".txt\" in file or \".md\" in file or \".doc\" in file):\n",
    "        files.append(file)\n",
    "\n",
    "with open(files[2]) as file:\n",
    "    text = file.read()\n",
    "\n",
    "redacted_words = redact_names(text)\n",
    "\n",
    "print(redacted_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
